{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hand Gesture Recognition using Logistic Regression\n",
                "\n",
                "This notebook demonstrates the complete pipeline for hand gesture recognition:\n",
                "1. Data loading and preprocessing\n",
                "2. Model training with Logistic Regression\n",
                "3. Model evaluation\n",
                "4. Real-time gesture recognition using webcam\n",
                "\n",
                "The dataset contains hand landmarks detected by MediaPipe, normalized and ready for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import classification_report"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading and Preprocessing\n",
                "\n",
                "Load the processed dataset containing hand landmarks and gesture labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"../data/processed/data.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Selection\n",
                "\n",
                "Select features for training. All columns except 'gesture' are features (63 landmark coordinates + handedness).\n",
                "The 'gesture' column is our target variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "feature_cols = [c for c in df.columns if c not in (\"gesture\")]\n",
                "\n",
                "X = df[feature_cols].values\n",
                "y = df[\"gesture\"].values"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Label Encoding\n",
                "\n",
                "Convert gesture class names to numerical labels using LabelEncoder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "le = LabelEncoder()\n",
                "\n",
                "y_encoded = le.fit_transform(y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train/Test Split\n",
                "\n",
                "Split the data into training and testing sets with stratification to maintain class distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Training\n",
                "\n",
                "Train a Logistic Regression model with multinomial classification using the SAGA solver.\n",
                "This configuration was chosen based on cross-validation results showing the highest accuracy and stability.\n",
                "\n",
                "Parameters:\n",
                "\n",
                "* `multi_class='multinomial'`: Suitable for multi-class gesture classification\n",
                "* `solver='saga'`: Efficient for larger datasets and supports multinomial loss with L2 regularization\n",
                "* `penalty='l2'`: Standard regularization to prevent overfitting\n",
                "* `C=10`: Low regularization to allow the model to fit complex patterns in normalized hand landmark data\n",
                "* `max_iter=1000`: Sufficient for convergence while keeping training time reasonable\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LogisticRegression(\n",
                "    multi_class='multinomial',\n",
                "    solver='saga',\n",
                "    penalty='l2',\n",
                "    C=10,\n",
                "    max_iter=1000,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "clf.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Evaluation\n",
                "\n",
                "Evaluate the trained model on the test set and print a detailed classification report."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = clf.predict(X_test)\n",
                "\n",
                "print(classification_report(y_test, y_pred, target_names=le.classes_))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Saving\n",
                "\n",
                "Save the trained model and label encoder to a pickle file for later use in inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "with open(\"gesture_lr.pkl\", \"wb\") as f:\n",
                "\n",
                "    pickle.dump({\"model\": clf, \"label_encoder\": le}, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Real-time Gesture Recognition\n",
                "\n",
                "Implement real-time gesture recognition using webcam feed and MediaPipe hand detection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import mediapipe as mp\n",
                "import numpy as np\n",
                "import pickle\n",
                "import time\n",
                "from collections import deque"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Trained Model\n",
                "\n",
                "Load the saved model and label encoder for inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"gesture_lr.pkl\", \"rb\") as f:\n",
                "    obj = pickle.load(f)\n",
                "\n",
                "clf = obj[\"model\"]\n",
                "le = obj[\"label_encoder\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize MediaPipe Hands\n",
                "\n",
                "Initialize MediaPipe Hands solution with appropriate parameters for real-time detection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mp_hands = mp.solutions.hands\n",
                "\n",
                "hands = mp_hands.Hands(\n",
                "    static_image_mode=False,\n",
                "    max_num_hands=1,\n",
                "    min_detection_confidence=0.8,\n",
                "    min_tracking_confidence=0.5,\n",
                "\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Webcam Capture\n",
                "\n",
                "Open webcam for video capture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cap = cv2.VideoCapture(0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## FPS Tracking Setup\n",
                "\n",
                "Initialize variables for FPS calculation and smoothing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "fps_history = deque(maxlen=10)\n",
                "last_time = time.time()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Recognition Loop\n",
                "\n",
                "Process video frames in real-time:\n",
                "1. Capture frame from webcam\n",
                "2. Calculate FPS\n",
                "3. Mirror the frame\n",
                "4. Process with MediaPipe\n",
                "5. Extract hand landmarks if detected\n",
                "6. Normalize coordinates\n",
                "7. Make prediction\n",
                "8. Display results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "while True:\n",
                "    ret, frame = cap.read()\n",
                "    if not ret:\n",
                "        break\n",
                "\n",
                "    now = time.time()\n",
                "    fps = 1.0 / (now - last_time)\n",
                "    last_time = now\n",
                "    fps_history.append(fps)\n",
                "    smooth_fps = sum(fps_history) / len(fps_history)\n",
                "\n",
                "    frame_mirrored = cv2.flip(frame, 1)\n",
                "    image_rgb = cv2.cvtColor(frame_mirrored, cv2.COLOR_BGR2RGB)\n",
                "    result = hands.process(image_rgb)\n",
                "\n",
                "    gesture_text = \"No hand\"\n",
                "    conf_text = \"\"\n",
                "\n",
                "    if result.multi_hand_landmarks and result.multi_handedness:\n",
                "        hand_landmarks = result.multi_hand_landmarks[0]\n",
                "        handedness_obj = result.multi_handedness[0].classification[0]\n",
                "        handedness = handedness_obj.label\n",
                "        confidence = handedness_obj.score \n",
                "        conf_text = f\"{confidence:.2f}\"\n",
                "\n",
                "        is_right = 1 if handedness == \"Right\" else 0\n",
                "\n",
                "        wrist_x = hand_landmarks.landmark[0].x\n",
                "        wrist_y = hand_landmarks.landmark[0].y\n",
                "        wrist_z = hand_landmarks.landmark[0].z\n",
                "\n",
                "\n",
                "        distances = [\n",
                "            np.linalg.norm([\n",
                "                lm.x - wrist_x,\n",
                "                lm.y - wrist_y,\n",
                "                lm.z - wrist_z\n",
                "            ])\n",
                "\n",
                "            for lm in hand_landmarks.landmark\n",
                "        ]\n",
                "\n",
                "        scale = max(distances)\n",
                "        coords = []\n",
                "\n",
                "        for lm in hand_landmarks.landmark:\n",
                "            x = (lm.x - wrist_x) / scale\n",
                "            y = (lm.y - wrist_y) / scale\n",
                "            z = (lm.z - wrist_z) / scale\n",
                "            coords.extend([x, y, z])\n",
                "\n",
                "\n",
                "\n",
                "        input_vec = np.array(coords + [is_right]).reshape(1, -1)\n",
                "        pred_class = clf.predict(input_vec)[0]\n",
                "        gesture_text = le.inverse_transform([pred_class])[0]\n",
                "\n",
                "\n",
                "\n",
                "    cv2.putText(frame_mirrored, f\"Gesture: {gesture_text}\",\n",
                "\n",
                "                (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (200, 200, 0), 3)\n",
                "\n",
                "    cv2.putText(frame_mirrored, f\"Conf: {conf_text}\",\n",
                "                (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 200, 255), 3)\n",
                "\n",
                "    cv2.putText(frame_mirrored, f\"FPS: {smooth_fps:.1f}\",\n",
                "                (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 200, 0), 3)\n",
                "\n",
                "\n",
                "    cv2.imshow(\"Gesture Recognition\", frame_mirrored)\n",
                "\n",
                "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
                "        break\n",
                "\n",
                "cap.release()\n",
                "hands.close()\n",
                "cv2.destroyAllWindows()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
